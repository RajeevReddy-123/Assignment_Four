{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/unt-iialab/INFO5731_Spring2020/blob/master/Assignments/INFO5731_Assignment_Four.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USSdXHuqnwv9"
      },
      "source": [
        "# **INFO5731 Assignment Four**\n",
        "\n",
        "In this assignment, you are required to conduct topic modeling, sentiment analysis based on **the dataset you created from assignment three**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWxodXh5n4xF"
      },
      "source": [
        "# **Question 1: Topic Modeling**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TenBkDJ5n95k"
      },
      "source": [
        "(30 points). This question is designed to help you develop a feel for the way topic modeling works, the connection to the human meanings of documents. Based on the dataset from assignment three, write a python program to **identify the top 10 topics in the dataset**. Before answering this question, please review the materials in lesson 8, especially the code for LDA, LSA, and BERTopic. The following information should be reported:\n",
        "\n",
        "(1) Features (text representation) used for topic modeling.\n",
        "\n",
        "(2) Top 10 clusters for topic modeling.\n",
        "\n",
        "(3) Summarize and describe the topic for each cluster. \n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rd7WDI_p5O7C",
        "outputId": "26385be8-ae12-4973-eaa5-ab08e9f58b71"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bertopic"
      ],
      "metadata": {
        "id": "l7CUTS__vezM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from bertopic import BERTopic\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "import numpy as np\n",
        "from sklearn.decomposition import LatentDirichletAllocation"
      ],
      "metadata": {
        "id": "AZrpXcAM4tXM"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "PuFPKhC0m1fd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58325eeb-a462-44c9-8211-158cb4c6a7fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 10 Clusters for Topic Modeling\n",
            "[(the, 0.16338758349588467), (and, 0.14166262933509235), (is, 0.13784149135357973), (on, 0.08097440971640042), (ios, 0.0758529435461054), (for, 0.06513840019093091), (battery, 0.06513840019093091), (to, 0.05951016021562593), (iphone, 0.05951016021562593), (it, 0.053670599369862036)]    1\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# load the dataset\n",
        "Apple_data = pd.read_csv('/content/drive/MyDrive/Assignment four/Apple_Review.csv')\n",
        "\n",
        "# set up a vectorizer to convert text data into numerical features\n",
        "Vectorizer = CountVectorizer(stop_words='english')\n",
        "\n",
        "# create a document-term matrix\n",
        "X = Vectorizer.fit_transform(Apple_data['clean_text'])\n",
        "\n",
        "# Initialize BERTopic model\n",
        "BERT_model = BERTopic()\n",
        "\n",
        "# Fit the model to the data\n",
        "topics, _ = BERT_model.fit_transform(Apple_data[\"clean_text\"])\n",
        "\n",
        "topics = BERT_model.get_topics()\n",
        "top10_topics = pd.Series(topics).value_counts().head(10)\n",
        "\n",
        "# printing the top 10 modeling\n",
        "print(\"Top 10 Clusters for Topic Modeling\")\n",
        "print(top10_topics)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Summarize and describe the topic for each cluster\n",
        "for i, (cluster, count) in enumerate(top10_topics.items()):\n",
        "    print(f\"Cluster {i+1}:\")\n",
        "    print(f\"Number of documents: {count}\")\n",
        "    cluster_id = cluster[0]\n",
        "    topic_words = BERT_model.get_topic(cluster_id)\n",
        "    if topic_words:\n",
        "        print(f\"Top words: {topic_words[:10]}\")\n",
        "        topic_docs = BERT_model.transform(Apple_data['clean_text'])\n",
        "        top_docs = np.argsort(np.array(topic_docs)[:, cluster_id])[::-1][:3]\n",
        "        print(f\"Sample Documents:\\n{Apple_data.iloc[top_docs]}\\n\")\n",
        "    else:\n",
        "        print(f\"No topics found for cluster {cluster_id}.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WgqEEDJZKEjY",
        "outputId": "270a15df-eec9-40ef-8992-4cb2f32ab8b4"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cluster 1:\n",
            "Number of documents: 1\n",
            "No topics found for cluster ('the', 0.16338758349588467).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AfpMRCrRwN6Z"
      },
      "source": [
        "# **Question 2: Sentiment Analysis**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dCQEbDawWCw"
      },
      "source": [
        "(30 points). Sentiment analysis also known as opinion mining is a sub field within Natural Language Processing (NLP) that builds machine learning algorithms to classify a text according to the sentimental polarities of opinions it contains, e.g., positive, negative, neutral. The purpose of this question is to develop a machine learning classifier for sentiment analysis. Based on the dataset from assignment three, write a python program to implement a sentiment classifier and evaluate its performance. Notice: **80% data for training and 20% data for testing**.  \n",
        "\n",
        "(1) Features used for sentiment classification and explain why you select these features.\n",
        "\n",
        "(2) Select two of the supervised learning algorithm from scikit-learn library: https://scikit-learn.org/stable/supervised_learning.html#supervised-learning, to build a sentiment classifier respectively. Note: Cross-validation (5-fold or 10-fold) should be conducted. Here is the reference of cross-validation: https://scikit-learn.org/stable/modules/cross_validation.html.\n",
        "\n",
        "(3) Compare the performance over accuracy, precision, recall, and F1 score for the two algorithms you selected. Here is the reference of how to calculate these metrics: https://towardsdatascience.com/accuracy-precision-recall-or-f1-331fb37c5cb9. "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can take into account the following features when classifying sentiment:\n",
        "\n",
        "**Text**: The textual substance of the reviews serves as the primary feature for sentiment classification. The cleaned text from the 'clean_text' column can be used as the input for our classifier. This feature is crucial since it contains data about user opinions and experiences that may be used to gauge the tone of the review.\n",
        "\n",
        "\n",
        "\n",
        "**Sentiment:** The user-assigned sentiment label is contained in the'sentiment' column. This label can be used as a feature by being converted into a numerical rating (e.g., positive = 1, negative = 0, neutral = 0.5). This feature can assist the classifier in learning the relationship between the review's textual content and the user-assigned sentiment label."
      ],
      "metadata": {
        "id": "RaaXCMyDyJ8l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Split the data into features and target variable\n",
        "X = Apple_data['clean_text']\n",
        "y = Apple_data['sentiment']\n",
        "\n",
        "# Vectorize the features using TF-IDF vectorizer\n",
        "Vectorizer = TfidfVectorizer()\n",
        "X = Vectorizer.fit_transform(X)\n",
        "\n",
        "# Build the logistic regression classifier\n",
        "lr_model = LogisticRegression()\n",
        "\n",
        "# Perform 5-fold cross-validation and print the evaluation metrics\n",
        "lr_scores = cross_validate(lr_model, X, y, cv=5, scoring=['accuracy', 'precision_macro', 'recall_macro', 'f1_macro'])\n",
        "\n",
        "print(\"Logistic Regression Scores:\")\n",
        "print(f\"Accuracy: {lr_scores['test_accuracy'].mean()*100:.2f}%\")\n",
        "print(f\"Precision: {lr_scores['test_precision_macro'].mean()*100:.2f}%\")\n",
        "print(f\"Recall: {lr_scores['test_recall_macro'].mean()*100:.2f}%\")\n",
        "print(f\"F1 Score: {lr_scores['test_f1_macro'].mean()*100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K0_9Gm2m4jbw",
        "outputId": "248a0c2b-98d8-49c6-e698-57e777de6085"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression Scores:\n",
            "Accuracy: 50.00%\n",
            "Precision: 25.00%\n",
            "Recall: 50.00%\n",
            "F1 Score: 33.33%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the random forest classifier\n",
        "rf_model = RandomForestClassifier()\n",
        "\n",
        "# Perform 5-fold cross-validation and print the evaluation metrics\n",
        "rf_scores = cross_validate(rf_model, X, y, cv=5, scoring=['accuracy', 'precision_macro', 'recall_macro', 'f1_macro'])\n",
        "print(\"\\nRandom Forest Scores:\")\n",
        "print(f\"Accuracy: {rf_scores['test_accuracy'].mean()*100:.2f}%\")\n",
        "print(f\"Precision: {rf_scores['test_precision_macro'].mean()*100:.2f}%\")\n",
        "print(f\"Recall: {rf_scores['test_recall_macro'].mean()*100:.2f}%\")\n",
        "print(f\"F1 Score: {rf_scores['test_f1_macro'].mean()*100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lw5rjZF8IVE-",
        "outputId": "35b7536b-7faf-4acf-f108-e9718cf0f374"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Random Forest Scores:\n",
            "Accuracy: 60.00%\n",
            "Precision: 45.00%\n",
            "Recall: 50.00%\n",
            "F1 Score: 46.67%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5mmYIfN8eYV"
      },
      "source": [
        "# **Question 3: House price prediction**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsi2y4z88ngX"
      },
      "source": [
        "(40 points). You are required to build a **regression** model to predict the house price with 79 explanatory variables describing (almost) every aspect of residential homes. The purpose of this question is to practice regression analysis, an supervised learning model. The training data, testing data, and data description files can be download from canvas. Here is an axample for implementation: https://towardsdatascience.com/linear-regression-in-python-predict-the-bay-areas-home-price-5c91c8378878. \n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load the training and testing data\n",
        "train_data = pd.read_csv('/content/drive/MyDrive/Assignment four/assignment4-question3-data.zip (Unzipped Files)/train.csv')\n",
        "test_data= pd.read_csv('/content/drive/MyDrive/Assignment four/assignment4-question3-data.zip (Unzipped Files)/test.csv')\n",
        "\n",
        "# Concatenate the training and testing data to ensure consistent preprocessing\n",
        "concat_data = pd.concat([train_data, test_data], sort=False)\n",
        "\n",
        "# Drop the target variable and the ID column\n",
        "target_data = concat_data.drop(['SalePrice', 'Id'], axis=1)\n",
        "\n",
        "# Handle missing values\n",
        "missing_data = target_data.fillna(target_data.mean())\n",
        "\n",
        "# One-hot encode categorical features\n",
        "encode_data = pd.get_dummies(missing_data)\n",
        "\n",
        "# Split the data back into training and testing sets\n",
        "train_set = encode_data[:len(train_data)]\n",
        "test_set = encode_data[len(train_data):]\n",
        "train_target = train_data['SalePrice']\n",
        "\n",
        "# Create a linear regression model and fit it to the training data\n",
        "lr_model = LinearRegression()\n",
        "lr_model.fit(train_set, train_target)\n",
        "\n",
        "# Make predictions on the testing set\n",
        "test_predictions = lr_model.predict(test_set)\n",
        "\n",
        "# Save the predictions to a CSV file\n",
        "House_submission= pd.DataFrame({'Id': test_data['Id'], 'SalePrice': test_predictions})\n",
        "House_submission.to_csv('House Price Prediction Sales.csv', index=False)\n"
      ],
      "metadata": {
        "id": "Iz4fFqES7JBK"
      },
      "execution_count": 66,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}